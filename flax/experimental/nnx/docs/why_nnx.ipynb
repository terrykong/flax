{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why NNX?\n",
    "\n",
    "Flax Linen is currently the most flexible and powerful way to write neural networks in JAX. The main features that have made it so popular are [State collections](https://flax.readthedocs.io/en/latest/glossary.html#term-Variable-collections), [RNG handling](https://flax.readthedocs.io/en/latest/glossary.html#term-RNG-sequences), [Collection-aware lifted transformations](https://flax.readthedocs.io/en/latest/developer_notes/lift.html), and [Leaf metadata](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning).\n",
    "\n",
    "However, Linen's power has come at a cost:\n",
    "* The `init` and `apply` APIs require a learning curve (on top of JAX's learning curve).\n",
    "* The Module's dataclass and `compact` semantics drift away from regular Python semantics and have a very complex internal implementation.\n",
    "* It is not very easily to integrate pre-trained models into bigger models as the Module structure is separate from the `params` structure.\n",
    "* The implementation of the lifted transformations is very complex.\n",
    "\n",
    "Flax NNX is an attempt to keep the features that made Linen great while simplifying the API and making it more Pythonic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## NNX is Pythonic\n",
    "* Example of building a Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from flax.experimental import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class Count(nnx.Variable): pass\n",
    "\n",
    "\n",
    "class Linear(nnx.Module):\n",
    "\n",
    "  def __init__(self, din: int, dout: int, *, ctx: nnx.Context):\n",
    "    self.din = din\n",
    "    self.dout = dout\n",
    "    key = ctx.make_rng(\"params\")\n",
    "    self.w = nnx.Param(jax.random.uniform(key, (din, dout)))\n",
    "    self.b = nnx.Param(jnp.zeros((dout,)))\n",
    "    self.count = Count(0)  # track the number of calls\n",
    "\n",
    "  def __call__(self, x) -> jax.Array:\n",
    "    self.count += 1\n",
    "    return x @ self.w + self.b\n",
    "\n",
    "\n",
    "model = Linear(din=5, dout=2, ctx=nnx.context(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.count = 1\n",
      "model.w = Array([[0.0779959 , 0.8061936 ],\n",
      "       [0.05617034, 0.55959475],\n",
      "       [0.3948189 , 0.5856023 ],\n",
      "       [0.82162833, 0.27394366],\n",
      "       [0.07696676, 0.8982161 ]], dtype=float32)\n",
      "model.b = Array([0., 0.], dtype=float32)\n",
      "model = Linear(\n",
      "  din=5,\n",
      "  dout=2\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x = jnp.ones((1, 5))\n",
    "y = model(x)\n",
    "\n",
    "print(f\"{model.count = }\")\n",
    "print(f\"{model.w = }\")\n",
    "print(f\"{model.b = }\")\n",
    "print(f\"{model = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## NNX's eager mode is simple\n",
    "* Example of training in eager mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss=0.2733\n",
      "Step 100: loss=0.0098\n",
      "Step 200: loss=0.0097\n",
      "Step 300: loss=0.0097\n",
      "Step 400: loss=0.0096\n",
      "\n",
      "model.w = Array([[0.7906097]], dtype=float32)\n",
      "model.b = Array([0.4034875], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.uniform(size=(1000, 1))\n",
    "Y = 0.8 * X + 0.4 + np.random.normal(scale=0.1, size=(1000, 1))\n",
    "\n",
    "model = Linear(1, 1, ctx=nnx.context(0))\n",
    "\n",
    "for step in range(500):\n",
    "  idx = np.random.randint(0, 1000, size=(32,))\n",
    "  x, y = X[idx], Y[idx]\n",
    "\n",
    "  def loss_fn(model: Linear):\n",
    "    y_pred = model(x)\n",
    "    return jnp.mean((y_pred - y) ** 2)\n",
    "\n",
    "  loss, grads = nnx.value_and_grad(loss_fn, wrt=nnx.Param)(model)\n",
    "\n",
    "  params = model.filter(nnx.Param)\n",
    "  params = jax.tree_map(lambda p, g: p - 0.1 * g, params, grads)\n",
    "  model.update_state(params)\n",
    "\n",
    "  if step % 100 == 0:\n",
    "    y_pred = model(X)\n",
    "    loss = np.mean((y_pred - Y) ** 2)\n",
    "    print(f\"Step {step}: loss={loss:.4f}\")\n",
    "\n",
    "print(f\"\\n{model.w = }\")\n",
    "print(f\"{model.b = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## NNX is friendly for advanced users\n",
    "* Example of manual scan over layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nnx.Module):\n",
    "\n",
    "  def __init__(self, dim: int, *, ctx: nnx.Context):\n",
    "    self.linear = nnx.Linear(dim, dim, ctx=ctx)\n",
    "    self.bn = nnx.BatchNorm(dim, ctx=ctx)\n",
    "    self.dropout = nnx.Dropout(0.5)\n",
    "\n",
    "  def __call__(self, x: jax.Array, *, ctx: nnx.Context) -> jax.Array:\n",
    "    x = self.linear(x)\n",
    "    x = self.bn(x, ctx=ctx)\n",
    "    x = self.dropout(x, ctx=ctx)\n",
    "    x = jax.nn.gelu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "class ScanMLP(nnx.Module):\n",
    "\n",
    "  def __init__(self, dim: int, *, n_layers: int, ctx: nnx.Context):\n",
    "    self.n_layers = n_layers\n",
    "    keys, ctxdef = ctx.partition()\n",
    "    params_key = jax.random.split(keys[\"params\"], n_layers)\n",
    "\n",
    "    @partial(jax.vmap, out_axes=(0, None, None))\n",
    "    def create_block(params_key):\n",
    "      ctx = ctxdef.merge({\"params\": params_key})\n",
    "      (params, batch_stats), moduledef = Block(dim, ctx=ctx).partition(\n",
    "          nnx.Param, nnx.BatchStat\n",
    "      )\n",
    "      return params, batch_stats, moduledef\n",
    "\n",
    "    params, batch_stats, moduledef = create_block(params_key)\n",
    "    self.layers = moduledef.merge(params, batch_stats)\n",
    "\n",
    "  def __call__(self, x: jax.Array, *, ctx: nnx.Context):\n",
    "    keys, ctxdef = ctx.partition()\n",
    "    dropout_key = jax.random.split(keys[\"dropout\"], self.n_layers)\n",
    "    (params, batch_stats), moduledef = self.layers.partition(\n",
    "        nnx.Param, nnx.BatchStat\n",
    "    )\n",
    "\n",
    "    def scan_fn(\n",
    "        carry: tuple[jax.Array, nnx.State], inputs: tuple[nnx.State, jax.Array]\n",
    "    ):\n",
    "      (x, batch_stats), (params, dropout_key) = carry, inputs\n",
    "      module = moduledef.merge(params, batch_stats)\n",
    "      x = module(x, ctx=ctxdef.merge({\"dropout\": dropout_key}))\n",
    "      params, _ = module.partition(nnx.Param)\n",
    "      return (x, batch_stats), params\n",
    "\n",
    "    (x, batch_stats), params = jax.lax.scan(\n",
    "        scan_fn, (x, batch_stats), (params, dropout_key)\n",
    "    )\n",
    "    self.layers.update_state((params, batch_stats))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parameter surgery is intuitive\n",
    "* Simple parameter surgery example\n",
    "\n",
    "## \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model():\n",
    "  ctx = nnx.context(0)\n",
    "  model = nnx.Sequence([\n",
    "      lambda x: x.reshape((x.shape[0], -1)),\n",
    "      nnx.Linear(784, 1024, ctx=ctx),\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape = (1, 10)\n",
      "state = State({\n",
      "  'backbone/1/bias': Param(\n",
      "    sharding=None,\n",
      "    value=(1024,)\n",
      "  ),\n",
      "  'backbone/1/kernel': Param(\n",
      "    sharding=None,\n",
      "    value=(784, 1024)\n",
      "  ),\n",
      "  'head/bias': Param(\n",
      "    sharding=None,\n",
      "    value=(10,)\n",
      "  ),\n",
      "  'head/kernel': Param(\n",
      "    sharding=None,\n",
      "    value=(1024, 10)\n",
      "  )\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nnx.Module):\n",
    "\n",
    "  def __init__(self, backbone: nnx.Sequence, *, ctx: nnx.Context):\n",
    "    self.backbone = backbone\n",
    "    self.head = nnx.Linear(1024, 10, ctx=ctx)\n",
    "\n",
    "  def __call__(self, x: jax.Array):\n",
    "    x = self.backbone(x)\n",
    "    x = self.head(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "pretrained_model = load_pretrained_model()\n",
    "model = Classifier(pretrained_model, ctx=nnx.context(0))\n",
    "y = model(jnp.ones((1, 28, 28)))\n",
    "\n",
    "print(\"y.shape =\", y.shape)\n",
    "print(\"state =\", jax.tree_map(jnp.shape, model.get_state()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacking Modules is possible\n",
    "* You can change the layers of an existing Module just by replacing the fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about Pytree-based libraries?\n",
    "* Equinox, Treex, [PytreeClass](https://github.com/ASEM000/PyTreeClass)\n",
    "* Shared mutable reference not allow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Ahead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
